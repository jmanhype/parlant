"""DSPy integration for optimizing guidelines."""
import logging
import time
from datetime import datetime
from typing import Any, Dict, List, Optional, Sequence, Union, ClassVar
from dataclasses import dataclass
from copy import deepcopy
import requests
import difflib

import dspy
from dspy import ChainOfThought, Example, LM, Module, Signature, Prediction, InputField, OutputField
from dspy.teleprompt import COPRO

from parlant.core.guidelines import Guideline, GuidelineContent
from parlant.core.metrics import ModelMetrics
from parlant.core.ollama import initialize_ollama_model

logger = logging.getLogger(__name__)

@dataclass
class OptimizationBatch:
    """A batch of guidelines to optimize together."""
    guidelines: List[Guideline]
    examples: List[Example]
    batch_size: int = 10
    start_time: Optional[float] = None
    end_time: Optional[float] = None

    @property
    def latency(self) -> Optional[float]:
        """Calculate batch processing latency in seconds."""
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        return None


class MetricsLogger:
    """Wrapper around language model to track metrics."""
    
    def __init__(self, lm: LM, metrics: ModelMetrics) -> None:
        """Initialize the metrics logger.
        
        Args:
            lm: Language model to wrap
            metrics: Metrics tracker
        """
        self.lm = lm
        self.metrics = metrics
        
    def __getattr__(self, name: str) -> Any:
        """Forward attribute access to wrapped language model.
        
        Args:
            name: Name of attribute to access
            
        Returns:
            Attribute value
        """
        # Get the attribute from the wrapped LM
        attr = getattr(self.lm, name)
        
        # If it's a method, wrap it to track metrics
        if callable(attr):
            def wrapper(*args: Any, **kwargs: Any) -> Any:
                self.metrics.total_api_calls += 1
                result = attr(*args, **kwargs)
                logger.info(f"Response format: {type(result)}")
                logger.info(f"Response content: {result}")
                if hasattr(result, 'usage') and hasattr(result.usage, 'total_tokens'):
                    self.metrics.total_tokens += result.usage.total_tokens
                elif hasattr(result, 'usage') and isinstance(result.usage, dict) and 'total_tokens' in result.usage:
                    self.metrics.total_tokens += result.usage['total_tokens']
                elif isinstance(result, list) and result and isinstance(result[0], dict) and 'usage' in result[0]:
                    for r in result:
                        if 'usage' in r and 'total_tokens' in r['usage']:
                            self.metrics.total_tokens += r['usage']['total_tokens']
                elif isinstance(result, dict) and 'usage' in result and 'total_tokens' in result['usage']:
                    self.metrics.total_tokens += result['usage']['total_tokens']
                elif isinstance(result, list) and result and isinstance(result[0], str):
                    # For DSPy responses that are lists of strings, estimate tokens
                    for r in result:
                        # Rough estimate: 1 token per 4 characters
                        self.metrics.total_tokens += len(r) // 4
                return result
            return wrapper
        return attr
        
    def __call__(self, *args: Any, **kwargs: Any) -> Any:
        """Handle direct calls to the language model.
        
        Args:
            *args: Positional arguments
            **kwargs: Keyword arguments
            
        Returns:
            Model output
        """
        self.metrics.total_api_calls += 1
        result = self.lm(*args, **kwargs)
        logger.info(f"Response format: {type(result)}")
        logger.info(f"Response content: {result}")
        if hasattr(result, 'usage') and hasattr(result.usage, 'total_tokens'):
            self.metrics.total_tokens += result.usage.total_tokens
        elif hasattr(result, 'usage') and isinstance(result.usage, dict) and 'total_tokens' in result.usage:
            self.metrics.total_tokens += result.usage['total_tokens']
        elif isinstance(result, list) and result and isinstance(result[0], dict) and 'usage' in result[0]:
            for r in result:
                if 'usage' in r and 'total_tokens' in r['usage']:
                    self.metrics.total_tokens += r['usage']['total_tokens']
        elif isinstance(result, dict) and 'usage' in result and 'total_tokens' in result['usage']:
            self.metrics.total_tokens += result['usage']['total_tokens']
        elif isinstance(result, list) and result and isinstance(result[0], str):
            # For DSPy responses that are lists of strings, estimate tokens
            for r in result:
                # Rough estimate: 1 token per 4 characters
                self.metrics.total_tokens += len(r) // 4
        return result


class OllamaAdapter(dspy.Adapter):
    """Adapter for using Ollama models with DSPy.
    
    This adapter wraps an Ollama model to make it compatible with DSPy's
    language model interface. It handles the communication with the Ollama
    server and formats responses appropriately.
    """
    
    def __init__(self, model_name: str) -> None:
        """Initialize the Ollama adapter.
        
        Args:
            model_name: Name of the Ollama model to use (e.g. 'ollama/llama2')
        """
        super().__init__()  # Initialize parent class
        self.model_name = model_name.split("/")[1]  # Extract model name after 'ollama/'
        
        # Set default kwargs for DSPy
        self.kwargs = {
            "temperature": 0.7,
            "max_tokens": 200,
            "top_p": 0.9,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "stop": None
        }
        
    def format(self, messages: List[Dict[str, str]]) -> str:
        """Format a list of messages into a prompt string.
        
        Args:
            messages: List of messages to format
            
        Returns:
            Formatted prompt string
        """
        formatted_messages = []
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            if role == "system":
                formatted_messages.append(f"System: {content}")
            elif role == "user":
                formatted_messages.append(f"User: {content}")
            elif role == "assistant":
                formatted_messages.append(f"Assistant: {content}")
                
        return "\n".join(formatted_messages)
        
    def parse(self, response: str) -> Dict[str, str]:
        """Parse a response string into a message dictionary.
        
        Args:
            response: Response string to parse
            
        Returns:
            Message dictionary with role and content
        """
        return {
            "role": "assistant",
            "content": self._clean_response(response)
        }
        
    def _clean_response(self, response: str) -> str:
        """Clean and format the response from Ollama.
        
        This method handles various response formats and ensures they are
        properly formatted for DSPy.
        
        Args:
            response: Raw response from Ollama
            
        Returns:
            Cleaned and formatted response string
        """
        # Remove any leading/trailing whitespace
        response = response.strip()
        
        # If response starts with '[', it's likely JSON-formatted
        if response.startswith("["):
            # Try to extract actual content
            try:
                # Look for actual content after the [
                content_start = response.find("[") + 1
                content_end = response.rfind("]")
                if content_start < content_end:
                    response = response[content_start:content_end].strip()
            except:
                pass
            
        # Remove any role prefixes
        for prefix in ["System:", "User:", "Assistant:"]:
            response = response.replace(prefix, "").strip()
        
        # If response is empty after cleaning, provide a default
        if not response:
            response = "I apologize, but I need more information to provide a helpful response."
            
        return response
    
    def basic_request(self, prompt: str, **kwargs: Any) -> Dict[str, str]:
        """Make a basic request to the Ollama model.
        
        Args:
            prompt: The prompt to send to the model
            **kwargs: Additional arguments for the request
            
        Returns:
            Dictionary containing the model's response
        """
        try:
            # Merge default kwargs with provided kwargs
            request_kwargs = self.kwargs.copy()
            request_kwargs.update(kwargs)
            
            # Make request to Ollama
            response = requests.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "temperature": request_kwargs["temperature"],
                    "top_p": request_kwargs["top_p"],
                    "max_tokens": request_kwargs["max_tokens"]
                }
            )
            response.raise_for_status()
            
            # Parse response
            result = response.json()
            cleaned_response = self._clean_response(result.get("response", ""))
            
            return {
                "content": cleaned_response
            }
            
        except Exception as e:
            logger.error(f"Failed to get response from Ollama: {e}")
            return {
                "content": "I apologize, but I am unable to process your request at the moment."
            }
    
    def __call__(self, prompt: str, **kwargs: Any) -> str:
        """Call the Ollama model with a prompt.
        
        This method is called by DSPy when using the adapter as a language model.
        
        Args:
            prompt: The prompt to send to the model
            **kwargs: Additional arguments for the request
            
        Returns:
            The model's response as a string
        """
        response = self.basic_request(prompt, **kwargs)
        return response["content"]


class CustomerServiceSignature(dspy.Signature):
    """A signature for generating customer service responses.
    
    This signature defines the input and output fields for generating
    professional and helpful customer service responses based on given
    conditions.
    """
    
    inputs: ClassVar[List[str]] = ['condition']
    
    condition: str = dspy.InputField(
        desc="The customer service condition to respond to",
        prefix="CONDITION: ",
        default=""
    )
    
    response: str = dspy.OutputField(
        desc="A helpful and professional response to the condition",
        prefix="RESPONSE: ",
        default=""
    )
    
    instructions: ClassVar[str] = """You are a professional customer service representative.

    Your task is to generate helpful, empathetic, and professional responses to customer service inquiries.
    
    Rules:
    1. Always be polite and professional
    2. Show empathy and understanding
    3. Provide clear and actionable information
    4. Ask for clarification when needed
    5. Maintain a positive and helpful tone
    
    Format:
    condition -> response
    
    Examples:
    'Customer asks about pricing' -> 'Thank you for your interest in our pricing. I'd be happy to explain our different pricing tiers. Could you please let me know what specific features or plans you're interested in?'
    
    'Customer reports a bug' -> 'I apologize for the inconvenience you're experiencing. To help resolve this issue, could you please provide more details about the bug? This will help us investigate and fix it as quickly as possible.'
    
    'Customer requests a refund' -> 'I understand you'd like a refund. I'll be happy to help you with this process. Could you please provide your order number and the reason for the refund request?'
    
    Let's solve this step by step:
    1. Understand the customer's condition
    2. Show empathy and acknowledge their concern
    3. Provide relevant information or ask for clarification
    4. Offer clear next steps or solutions
    5. End with a professional and helpful tone
    """


class CustomerServiceProgram(dspy.Module):
    """A DSPy module that generates customer service responses based on conditions.
    
    This module uses a predictor to generate contextually appropriate
    and professional responses to customer service inquiries.
    """

    def __init__(self) -> None:
        """Initialize the CustomerServiceProgram with a predictor."""
        super().__init__()
        
        # Create the predictor with our signature
        self.predictor = dspy.Predict(CustomerServiceSignature)

    def forward(self, **inputs: Any) -> Dict[str, str]:
        """Generate a customer service response based on a condition.
        
        Args:
            **inputs: Input dictionary containing 'condition' key
            
        Returns:
            Dict containing the generated response
            
        Raises:
            ValueError: If condition is not provided or not a string
        """
        # Step 1: Get condition from inputs
        condition = inputs.get('condition')
        if not condition or not isinstance(condition, str):
            raise ValueError("Must provide 'condition' as a string")
        
        # Step 2: Process using predictor
        result = self.predictor(**inputs)
        
        # Step 3: Return response
        return {"response": result.response}

    def deepcopy(self) -> 'CustomerServiceProgram':
        """Create a deep copy of this program.
        
        Returns:
            A new instance of CustomerServiceProgram
        """
        return CustomerServiceProgram()

    def predictors(self) -> List[dspy.Predict]:
        """Get the predictors used by this program.
        
        Returns:
            A list containing the predictor
        """
        return [self.predictor]


class CustomCOPRO(dspy.teleprompt.COPRO):
    """A custom COPRO optimizer that handles extended_signature gracefully.
    
    This subclass overrides the behavior of setting extended_signature to avoid
    the error in DSPy 2.5.
    """
    
    def compile(
        self, 
        student: CustomerServiceProgram, 
        *, 
        trainset: List[dspy.Example],
        eval_kwargs: Dict[str, Any]
    ) -> Optional[CustomerServiceProgram]:
        """Compile the program with training examples.
        
        This method overrides the parent's compile method to handle the
        extended_signature attribute gracefully.
        
        Args:
            student: The program to optimize
            trainset: List of training examples
            eval_kwargs: Additional arguments for evaluation
            
        Returns:
            The optimized program, or None if optimization fails
        """
        try:
            # Try to compile normally
            return super().compile(student, trainset=trainset, eval_kwargs=eval_kwargs)
        except AttributeError as e:
            if "extended_signature" in str(e):
                # If the error is about extended_signature, just return the original program
                logger.warning("COPRO optimization failed due to extended_signature, using original program")
                return student
            raise


class BatchOptimizedGuidelineManager:
    """A manager for optimizing and generating customer service responses.
    
    This class uses DSPy's COPRO optimizer to improve response quality and
    handles batched optimization of guidelines. It supports both OpenAI and
    Ollama models, with appropriate configuration for each.
    
    Attributes:
        metrics: Metrics tracker for monitoring model performance
        use_optimizer: Whether to use DSPy's COPRO optimizer
        lm: Language model instance for generating responses
        program: Base program for generating customer service responses
    """
    
    def __init__(
            self,
            api_key: Optional[str] = None,
            model_name: str = "openai/gpt-3.5-turbo",
            metrics: Optional[ModelMetrics] = None,
            use_optimizer: bool = True
        ) -> None:
        """Initialize the guideline manager.
        
        Args:
            api_key: Optional API key for the model provider
            model_name: Name of the model to use (e.g. 'openai/gpt-3.5-turbo' or 'ollama/llama2')
            metrics: Optional metrics tracker for monitoring model performance
            use_optimizer: Whether to use DSPy's COPRO optimizer for improving responses
            
        Raises:
            ValueError: If an invalid model name is provided
            RuntimeError: If initialization of the language model fails
        """
        self.metrics = metrics or ModelMetrics()
        self.use_optimizer = use_optimizer
        
        try:
            # Configure language model
            if "ollama" in model_name:
                # For Ollama models, use our custom adapter
                ollama_model = model_name.split("/")[1]
                initialize_ollama_model(ollama_model)
                self.lm = OllamaAdapter(model_name)
            else:
                # For other models like OpenAI, use all parameters
                self.lm = LM(model_name, api_key=api_key, temperature=0.7, max_tokens=200)
                
            if metrics:
                self.lm = MetricsLogger(self.lm, metrics)
                
            # Configure DSPy with the language model
            dspy.configure(lm=self.lm)
            
            # Create base program
            self.program = CustomerServiceProgram()
            
        except Exception as e:
            logger.error(f"Failed to initialize guideline manager: {e}")
            raise RuntimeError(f"Failed to initialize guideline manager: {e}") from e

    def optimize_guidelines(
            self, 
            guidelines: Sequence[Guideline],
            examples: List[Dict[str, str]],
            batch_size: int = 10
        ) -> List[Guideline]:
        """Optimize guidelines using DSPy's COPRO optimizer.
        
        This method takes a list of guidelines and example responses, then uses
        COPRO to optimize the response generation process. Guidelines are processed
        in batches to improve efficiency.
        
        Args:
            guidelines: List of guidelines to optimize
            examples: List of example dictionaries with 'condition' and 'response' keys
            batch_size: Number of guidelines to optimize at once
            
        Returns:
            List of optimized guidelines with improved responses
        """
        optimized_guidelines = []
        
        for i in range(0, len(guidelines), batch_size):
            batch = guidelines[i:i + batch_size]
            
            # Create training examples
            train_examples = []
            for example in examples:
                # Create example with proper input/output fields
                ex = dspy.Example(
                    condition=example['condition'],
                    response=example['response']
                ).with_inputs('condition')  # Explicitly set which fields are inputs
                train_examples.append(ex)
            
            try:
                # Configure COPRO optimizer with evaluation metric
                copro = CustomCOPRO(
                    prompt_model=self.lm,
                    metric=self._calculate_response_quality,
                    breadth=5,  # Number of new prompts to generate at each iteration
                    depth=3,  # Number of optimization iterations
                    init_temperature=1.4  # Temperature for generating new prompts
                )
                
                # Compile the program with training examples
                optimized_program = copro.compile(
                    student=self.program,
                    trainset=train_examples,
                    eval_kwargs={}  # Metric is already passed to COPRO constructor
                )
                
                # Generate responses for each guideline
                if optimized_program:
                    for guideline in batch:
                        try:
                            # Generate response using optimized program
                            pred = optimized_program(condition=guideline.content.condition)
                            
                            # Extract response from prediction
                            response = pred.response if hasattr(pred, 'response') else pred['response']
                            
                            # Create new guideline with updated response
                            optimized_guidelines.append(Guideline(
                                id=guideline.id,
                                creation_utc=guideline.creation_utc,
                                content=GuidelineContent(
                                    condition=guideline.content.condition,
                                    action=response
                                )
                            ))
                        except Exception as e:
                            logger.warning(f"Failed to optimize guideline: {str(e)}")
                            # Fall back to original program
                            pred = self.program(condition=guideline.content.condition)
                            response = pred.response if hasattr(pred, 'response') else pred['response']
                            
                            # Create new guideline with updated response
                            optimized_guidelines.append(Guideline(
                                id=guideline.id,
                                creation_utc=guideline.creation_utc,
                                content=GuidelineContent(
                                    condition=guideline.content.condition,
                                    action=response
                                )
                            ))
                else:
                    logger.warning("No valid candidates found during optimization, using original program")
                    for guideline in batch:
                        pred = self.program(condition=guideline.content.condition)
                        response = pred.response if hasattr(pred, 'response') else pred['response']
                        
                        # Create new guideline with updated response
                        optimized_guidelines.append(Guideline(
                            id=guideline.id,
                            creation_utc=guideline.creation_utc,
                            content=GuidelineContent(
                                condition=guideline.content.condition,
                                action=response
                            )
                        ))
                        
            except Exception as e:
                logger.warning(f"Failed to optimize batch: {str(e)}")
                for guideline in batch:
                    pred = self.program(condition=guideline.content.condition)
                    response = pred.response if hasattr(pred, 'response') else pred['response']
                    
                    # Create new guideline with updated response
                    optimized_guidelines.append(Guideline(
                        id=guideline.id,
                        creation_utc=guideline.creation_utc,
                        content=GuidelineContent(
                            condition=guideline.content.condition,
                            action=response
                        )
                    ))
                
        return optimized_guidelines

    def _calculate_response_quality(
        self, 
        example: dspy.Example, 
        pred: Union[dspy.Prediction, Dict[str, str]]
    ) -> float:
        """Calculate the quality score for a predicted response.
        
        This method evaluates the quality of a predicted response by comparing it
        to the expected response in the example.
        
        Args:
            example: The example containing the expected response
            pred: Dictionary or Prediction containing the predicted response
            
        Returns:
            A quality score between 0 and 1, where higher is better
        """
        try:
            # Extract expected and predicted responses
            expected_response = example.response
            predicted_response = pred.response if hasattr(pred, 'response') else pred['response']
            
            # Simple string matching for now
            # TODO: Use more sophisticated metrics like semantic similarity
            if predicted_response and expected_response:
                # Calculate basic string similarity
                similarity = difflib.SequenceMatcher(None, predicted_response, expected_response).ratio()
                return similarity
            
            return 0.0
            
        except Exception as e:
            logger.error(f"Error calculating response quality: {str(e)}")
            return 0.0
